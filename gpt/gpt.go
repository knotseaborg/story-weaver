package gpt

import (
	"encoding/json"
	"fmt"
	"log"
	"os"
	"strings"

	"github.com/knotseaborg/wikiSearchServer/common"
)

func GenerateQueryPlan(queryIntent, reference string) *QueryPlan {
	/*
		GenerateQueryPlan generates a query plan using a query intent and a reference.

		Parameters:
		  queryIntent: The query intent used to generate the query plan.
		  reference:   The reference information relevant to the query intent.

		Returns:
		  *QueryPlan: A pointer to the generated query plan.

		Example:
		  queryIntent := "List of companies headquartered in New York City"
		  reference := "List of Fortune 500 companies in 2023"
		  plan := GenerateQueryPlan(queryIntent, reference)
		  fmt.Println("Generated Query Plan:", plan)
	*/
	// Read the few shot samples
	sample, err := os.ReadFile("gpt/prompts/sparQL.txt")
	if err != nil {
		log.Fatal("Error reading file", err)
	}
	if reference == "" {
		reference = "None"
	}
	// Generate query plan
	prompt := fmt.Sprintf("%s\nREFERENCE:%s\nINPUT:%s\nOUTPUT:\n", sample, reference, queryIntent)
	prompt = common.CleanForJSON(prompt)
	resp, err := Completion(prompt, os.Getenv("GPT_MODEL_ADVANCED"))
	var plan QueryPlan
	err = json.Unmarshal([]byte(resp), &plan)
	if err != nil {
		log.Println(resp)
		log.Fatal("Error unmarshalling response: ", err)
	}
	return &plan
}

func IsAnswerableFromHistory(queryIntent, context string) bool {
	/*
		IsAnswerableFromHistory determines if the query intent can be answered based on historical context.

		Parameters:
		  queryIntent: The query intent to be checked for answerability.
		  context:     The historical context relevant to the query intent.

		Returns:
		  bool:  True if the query intent is answerable based on the historical context, false otherwise.

		Example:
		  isAnswerable := IsAnswerableFromHistory("How did the company perform in Q4?", "Reference: Company financial report for Q4 2023.")
		  if !isAnswerable {
		      log.Println("Query intent is not answerable based on historical context.")
		  }
	*/
	prompt := fmt.Sprintf("Reference:\n%s\nCan you guess an answer this question?\n%sOnly respond with a \"YES\" or \"NO\" and then justify your answer.\n", context, queryIntent)
	prompt = common.CleanForJSON(prompt)
	resp, err := Completion(prompt, os.Getenv("GPT_MODEL_BASIC"))
	if err != nil {
		log.Fatal("Error Classfying source")
	}
	if strings.ToLower(resp[:2]) == "no" { // We expect a yes or no response, followed by justification.
		log.Println("Justification:", resp)
		return false
	}
	return true
}

func GenerateQueryIntents(text string) ([]string, error) {
	/*
		GenerateQueryIntents generates query intents by providing a text prompt to the GPT model.

		Parameters:
		  text: Text to be appended to the base prompt for generating query intents.

		Returns:
		  []string: List of query intents generated by the GPT model.
		  error:    An error if the query intent generation process fails.

		Example:
		  intents, err := GenerateQueryIntents("What are the best restaurants in New York?")
		  if err != nil {
		      log.Fatal("Error generating query intents:", err)
		  }
		  fmt.Println("Query intents:", intents)
	*/
	content, err := os.ReadFile("gpt/prompts/intent.txt")
	if err != nil {
		log.Fatal(err)
	}
	prompt := fmt.Sprintf("%s\n%s", string(content), text)
	prompt = common.CleanForJSON(prompt)
	resp, err := Completion(prompt, os.Getenv("GPT_MODEL_ADVANCED"))
	if err != nil {
		log.Fatal("Error building query", err)
	}
	intents := []string{}
	for _, intent := range strings.Split(resp, "\n") {
		intent = strings.Trim(intent, " \n")
		if len(intent) > 0 {
			intents = append(intents, intent)
		}
	}
	return intents, nil
}

func Completion(text string, model string) (string, error) {
	/*
		Completion generates a chat completion using the OpenAI GPT model.

		Parameters:
		  text:   Text prompt to generate completion.
		  model:  Name of the model to use for completion.

		Returns:
		  string: Completed text generated by the GPT model.
		  error:  An error if the completion process fails.

		Example:
		  completedText, err := Completion("How are you?", "gpt-3.5-turbo")
		  if err != nil {
		      log.Fatal("Error completing text:", err)
		  }
		  fmt.Println("Completed text:", completedText)
	*/
	payload := []byte(fmt.Sprintf(`{
	"model": "%s",
	"temperature": %s,
	"messages": [
	  {
	    "role": "user",
	    "content": "%s"
	  }
	]
	}`, model, os.Getenv("GPT_TEMPERATURE"), text))

	byteContent, err := common.RequestPOST(os.Getenv("GPT_URL"), payload)
	if err != nil {
		log.Fatal("Error while GPT completion", err)
	}
	var comp completion
	err = json.Unmarshal(byteContent, &comp)
	if err != nil {
		log.Println("Error completion: ", err)
		return "", err
	}
	return comp.Choices[0].Message.Content, nil
}
